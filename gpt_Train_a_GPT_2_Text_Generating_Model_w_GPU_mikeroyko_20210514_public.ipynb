{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tedk108/GPT-2-Journalism-Can-AI-produce-Mike-Royko-s-writing-/blob/main/gpt_Train_a_GPT_2_Text_Generating_Model_w_GPU_mikeroyko_20210514_public.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "#  Train a GPT-2 Text-Generating Model w/ GPU For Free \n",
        "\n",
        "by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "*Last updated: February 14th, 2021*\n",
        "\n",
        "Retrain an advanced text generating neural network on any text dataset **for free on a GPU using Collaboratory** using `gpt-2-simple`!\n",
        "\n",
        "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). You can also read my [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n",
        "\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n",
        "2. Make sure you're running the notebook in Google Chrome.\n",
        "3. Run the cells below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBkpRgBCBS2_",
        "outputId": "2e874106-2638-43e6-ba4f-7466707b309a"
      },
      "source": [
        "%%time\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 12.2 ms, sys: 36.4 ms, total: 48.6 ms\n",
            "Wall time: 2.38 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80/P4 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8wSlgXoDPCR",
        "outputId": "1d8edb08-5def-4e4c-8380-bfdbb4be3e15"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 1m25s at  4:22 on 20210414 to download 124M model\n",
        "#       8m00s at 17:30 on 20210419 to download 355M model\n",
        "\n",
        "gpt2.download_gpt2(model_name=\"355M\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 459Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 3.63Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 360Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [03:26, 6.86Mit/s]                                 \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 229Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 3.45Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 4.40Mit/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5.03 s, sys: 3.74 s, total: 8.77 s\n",
            "Wall time: 3min 29s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUkSmzvDCzvl"
      },
      "source": [
        "## Upload your fine tuning textfile.txt to the temporary Colab drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCVULp8uApWn"
      },
      "source": [
        "# DRAG AND DROP your clean corpus.txt file onto the temporary virtual Colab drive as shown in the class video\n",
        "\n",
        "# Your filename should have no spaces, only alphanumeric and underscore characters with a '.txt' file extension\n",
        "\n",
        "# e.g. 'fitzgerald_sarahg.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d20Uw5rwvy9n",
        "outputId": "fe5f8d53-c3f9-4d83-b17d-b5ef2d43434c"
      },
      "source": [
        "# Verify that you can see your uploaded file here\n",
        "\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint  mike_royko_new.txt\tmike_royko.txt\tmodels\tsample_data  samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7e_mfbpeOtp",
        "outputId": "f13174fc-fa51-4c81-8243-e7caa26e05cb"
      },
      "source": [
        "# Verify the content of the first 25 lines\n",
        "\n",
        "!head -n 25 mike_royko.txt  # Change 'shakespeare.txt' to the name of your file (e.g. 'fitzgerald_sarahg.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1995 \r\n",
            "Enquiring minds don't need to know \r\n",
            "Tuesday, January 3, 1995 \r\n",
            "When we met for our traditional New Year's drink of Ovaltine, Slats Grobnik said: \"Tell me about those pills. You buy them across the counter or does a doc have to write a prescription?\" \r\n",
            "Pills? What pills? \"Those Stupid Pills I figure you been taking lately. Boy, they really did the job, didn't they?\" I am not familiar with Stupid Pills and have not used them. \"You did it all on your own? Boy, then you're a natural like that baseball movie with Robert Redford. Maybe they'll make a \r\n",
            "movie like that about you, except at the end the only fireworks will be in your head.\" Do you mind if we talk about something else? \"Hey, no problem.\" Thank you. I gather that you, too, have been on vacation. Did you have a pleasant time? \"Sure. And I didn't get arrested even once.\" \r\n",
            "That wasn't what I meant. Did you go anywhere? \"Yeah, I took a little trip. You wanna hear about it?\" Sure. \"Well, I got where I was going without having to put up any bail money.\" Forget it. \r\n",
            "\"Just making conversation. But I guess you don't wanna talk about it, huh?\" About what? \"How you got to be public enemy No. 1.\" That's a slight exaggeration. But, no, my attorney advised me against saying anything until we go to court. \"Oh, that ought to be fun with the cameras being shoved in your face and the TV reporters asking you how you feel, and \r\n",
            "if you regret being a jerk, and if you're ever going to do it again, and if you are thinking about hanging yourself, and how now you ain't got no credibility no more, and are you going to get in another line of work, and did your boss say you ought to go to the Betty Ford clinic, and how you are going to get around without wheels, and is your wife going to get a divorce, and how are you ever going to write something bad about someone else when now you are such a bum, and if you decide to hang yourself will you do it where they can get a good shot for the 4 o'clock news, and . . .\" \r\n",
            "Excuse me, I said I would rather not discuss it. \"But you ain't; I am. When you come out of the courthouse, you gonna take a swing?\" At who? \"The reporters or the camera guys. Even if the punch don't land, it would make a great bite for the 4 o'clock news, and they'd \r\n",
            "probably use it again at 10. You might even make the networks again.\" Of course not. Why would I do something like that? \"Well, as long as you're taking those Stupid Pills, you might as well get all the benefits.\" I told you, I am not taking any such pills. And let us change the subject, please. How are things at work? \"Just fine. And when I pull into the company parking lot, everybody don't run to call their insurance agents, and nobody \r\n",
            "asks me if I can walk a straight line and touch my nose. Speaking of that, with your nose, you should have been able to pass the test by touching it with your foot.\" \r\n",
            "And how is the family? \"Everybody's fine. Why not? When they go out, they don't have to wear bags over their faces because of me. How about \r\n",
            "yours? Have they tried to have you committed yet?\" No, but it's nice of you to ask. \"What about Oprah or Geraldo?\" I don't watch their shows, but what about them? \"Have you thought about going on? I'm sure they'd be glad to have you even if you didn't wear a sequined gown and high \r\n",
            "heels.\" Why in the world would I do that? \"Geez, to talk about your rise and fall - how you went from being a highly respected bum to a pathetic, sad, down-and- \r\n",
            "out bum. They'd probably bring on some other once-mighty bums - maybe ol' Spiro Agnew, Darryl Strawberry the ballplayer, and that guy Keating from the S&L scandal - so you could compare notes on how you turned into social outcasts. Then the audience could ask questions about how it feels to be such lousy lowlifes. \r\n",
            "\"It could be a heck of a show, and you could get a tape to leave to your grandchildren so they'll know what a live wire you were.\" \r\n",
            "Thanks for the suggestion, but I believe that I'll decline the honor and instead go through the normal legal process. \"That's all? Just go to court?\" That's customary, isn't it? \"I guess so. But it don't seem like enough. Hey, how's about if I go rent a white Bronco with a car phone, then we can go for \r\n",
            "a ride on the expressways, and I'll drive, and you can announce that you're going to end it all, and . . .\" How about if we just have another Ovaltine. \"You're no fun.\" Yes, and it's about time. \r\n",
            "Voices of gloom as close as radio \r\n",
            "Wednesday, January 4, 1995 \r\n",
            "For most people, waking up in a cheerful mood is a fine way to begin the day. But not in my line of work. \r\n",
            "Feeling bright and chipper could cause me to lose control and irresponsibly write something bright and chipper. And that would turn the stomachs of regular readers, thereby ruining their day too. \r\n",
            "Fortunately, that's seldom a problem. I was blessed with a naturally glum temperament that views each bright dawn as a potential disaster. When the golden sunlight streams through the window, my reaction is: \"That hurts my eyes.\" Maybe I have a Transylvanian ancestor. \r\n",
            "But once in a while it happens. I awake feeling cheerful. Maybe it is the result of a chemical imbalance. Just about every- thing is. And I can only hope that science will someday come up with an anti-happy pill for those of us who suffer from this affliction. Until then, I have to do whatever it takes to shake the dreaded mood or it festers all day. \r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUVFYKWav_R8"
      },
      "source": [
        "# Change \"shakespeare.txt\" to your filename \n",
        "# e.g. \n",
        "# file_name = 'harry_potter.txt'\n",
        "\n",
        "file_name = \"mike_royko.txt\"  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDHnTCNghf9T",
        "outputId": "7e4eaa63-9400-4581-a3ae-d9eca2c593b5"
      },
      "source": [
        "# Read/Write file to force convert to known/legal encoding\n",
        "\n",
        "import io\n",
        "with io.open(file_name, \"r\", encoding=\"utf-8\", errors='ignore') as my_file:\n",
        "     my_unicode_str = my_file.read() \n",
        "\n",
        "print(my_unicode_str[:5000])\n",
        "\n",
        "file_name_new = file_name.split('.')[0] + '_new.txt'\n",
        "\n",
        "with io.open(file_name_new, \"w\", encoding=\"utf-8\", errors='ignore') as my_file:\n",
        "     my_file.write(my_unicode_str) \n",
        "\n",
        "print(f'\\n==========\\n\\n[Just wrote out new cleaned filename]: {file_name_new}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1995 \n",
            "Enquiring minds don't need to know \n",
            "Tuesday, January 3, 1995 \n",
            "When we met for our traditional New Year's drink of Ovaltine, Slats Grobnik said: \"Tell me about those pills. You buy them across the counter or does a doc have to write a prescription?\" \n",
            "Pills? What pills? \"Those Stupid Pills I figure you been taking lately. Boy, they really did the job, didn't they?\" I am not familiar with Stupid Pills and have not used them. \"You did it all on your own? Boy, then you're a natural like that baseball movie with Robert Redford. Maybe they'll make a \n",
            "movie like that about you, except at the end the only fireworks will be in your head.\" Do you mind if we talk about something else? \"Hey, no problem.\" Thank you. I gather that you, too, have been on vacation. Did you have a pleasant time? \"Sure. And I didn't get arrested even once.\" \n",
            "That wasn't what I meant. Did you go anywhere? \"Yeah, I took a little trip. You wanna hear about it?\" Sure. \"Well, I got where I was going without having to put up any bail money.\" Forget it. \n",
            "\"Just making conversation. But I guess you don't wanna talk about it, huh?\" About what? \"How you got to be public enemy No. 1.\" That's a slight exaggeration. But, no, my attorney advised me against saying anything until we go to court. \"Oh, that ought to be fun with the cameras being shoved in your face and the TV reporters asking you how you feel, and \n",
            "if you regret being a jerk, and if you're ever going to do it again, and if you are thinking about hanging yourself, and how now you ain't got no credibility no more, and are you going to get in another line of work, and did your boss say you ought to go to the Betty Ford clinic, and how you are going to get around without wheels, and is your wife going to get a divorce, and how are you ever going to write something bad about someone else when now you are such a bum, and if you decide to hang yourself will you do it where they can get a good shot for the 4 o'clock news, and . . .\" \n",
            "Excuse me, I said I would rather not discuss it. \"But you ain't; I am. When you come out of the courthouse, you gonna take a swing?\" At who? \"The reporters or the camera guys. Even if the punch don't land, it would make a great bite for the 4 o'clock news, and they'd \n",
            "probably use it again at 10. You might even make the networks again.\" Of course not. Why would I do something like that? \"Well, as long as you're taking those Stupid Pills, you might as well get all the benefits.\" I told you, I am not taking any such pills. And let us change the subject, please. How are things at work? \"Just fine. And when I pull into the company parking lot, everybody don't run to call their insurance agents, and nobody \n",
            "asks me if I can walk a straight line and touch my nose. Speaking of that, with your nose, you should have been able to pass the test by touching it with your foot.\" \n",
            "And how is the family? \"Everybody's fine. Why not? When they go out, they don't have to wear bags over their faces because of me. How about \n",
            "yours? Have they tried to have you committed yet?\" No, but it's nice of you to ask. \"What about Oprah or Geraldo?\" I don't watch their shows, but what about them? \"Have you thought about going on? I'm sure they'd be glad to have you even if you didn't wear a sequined gown and high \n",
            "heels.\" Why in the world would I do that? \"Geez, to talk about your rise and fall - how you went from being a highly respected bum to a pathetic, sad, down-and- \n",
            "out bum. They'd probably bring on some other once-mighty bums - maybe ol' Spiro Agnew, Darryl Strawberry the ballplayer, and that guy Keating from the S&L scandal - so you could compare notes on how you turned into social outcasts. Then the audience could ask questions about how it feels to be such lousy lowlifes. \n",
            "\"It could be a heck of a show, and you could get a tape to leave to your grandchildren so they'll know what a live wire you were.\" \n",
            "Thanks for the suggestion, but I believe that I'll decline the honor and instead go through the normal legal process. \"That's all? Just go to court?\" That's customary, isn't it? \"I guess so. But it don't seem like enough. Hey, how's about if I go rent a white Bronco with a car phone, then we can go for \n",
            "a ride on the expressways, and I'll drive, and you can announce that you're going to end it all, and . . .\" How about if we just have another Ovaltine. \"You're no fun.\" Yes, and it's about time. \n",
            "Voices of gloom as close as radio \n",
            "Wednesday, January 4, 1995 \n",
            "For most people, waking up in a cheerful mood is a fine way to begin the day. But not in my line of work. \n",
            "Feeling bright and chipper could cause me to lose control and irresponsibly write something bright and chipper. And that would turn the stomachs of regular readers, thereby ruining their day too. \n",
            "Fortunately, that's seldom a problem. I was blessed with a naturally glum temperament that views each bright dawn as a potential disaster. When the golden sunlight streams through the window, my reaction is: \"That hurts my eyes.\" Maybe I h\n",
            "\n",
            "==========\n",
            "\n",
            "[Just wrote out new cleaned filename]: mike_royko_new.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fNu6nyzxaAp"
      },
      "source": [
        "# %reset_selective sess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6cIwpf4GZbB",
        "outputId": "d8f99db5-fce2-45f0-fb25-c3ca58d9dbe1"
      },
      "source": [
        "%whos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variable         Type             Data/Info\n",
            "-------------------------------------------\n",
            "datetime         type             <class 'datetime.datetime'>\n",
            "file_name        str              mike_royko.txt\n",
            "file_name_new    str              mike_royko_new.txt\n",
            "files            module           <module 'google.colab.fil<...>s/google/colab/files.py'>\n",
            "gpt2             module           <module 'gpt_2_simple' fr<...>pt_2_simple/__init__.py'>\n",
            "io               module           <module 'io' from '/usr/lib/python3.7/io.py'>\n",
            "my_file          TextIOWrapper    <_io.TextIOWrapper name='<...>ode='w' encoding='utf-8'>\n",
            "my_unicode_str   str              1995 \\nEnquiring minds do<...>per, this town, weeps. \\n\n",
            "sess             Session          <tensorflow.python.client<...>object at 0x7fc5940e5c90>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWY8Zy_LyIIK"
      },
      "source": [
        "# import tensorflow as tf\n",
        "# tf.reset_default_graph()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "aeXshJM-Cuaf",
        "outputId": "a120a89b-03bb-4048-bc79-cde8b6aa8ef2"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 21m46s at 4:23 on 20210414 with 124M model\n",
        "#       11m05s at 17:30 on 20210419 with 355M model\n",
        "#              at  5;15 on 20210420 with 355M model/fitzgerald_sarahg.txt\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name_new,\n",
        "              model_name='355M',    # Change this if you use a different sized model\n",
        "              steps=2000,           # Number of training epochs\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,       # Print feedback of loss/avg metrics every 10 epochs\n",
        "              sample_every=200,     # Print sample text generation every 200 epochs to see progress of model learning\n",
        "              save_every=500        # Save learned weights/biases every 500 epochs in case of crash/interruptions\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-3c6512ea0996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n# NOTE: 21m46s at 4:23 on 20210414 with 124M model\\n#       11m05s at 17:30 on 20210419 with 355M model\\n#              at  5;15 on 20210420 with 355M model/fitzgerald_sarahg.txt\\n\\nsess = gpt2.start_tf_sess()\\n\\ngpt2.finetune(sess,\\n              dataset=file_name_new,\\n              model_name='355M',    # Change this if you use a different sized model\\n              steps=2000,           # Number of training epochs\\n              restore_from='fresh',\\n              run_name='run1',\\n              print_every=10,       # Print feedback of loss/avg metrics every 10 epochs\\n              sample_every=200,     # Print sample text generation every 200 epochs to see progress of model learning\\n              save_every=500        # Save learned weights/biases every 500 epochs in case of crash/interruptions\\n              )\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(sess, dataset, steps, model_name, model_dir, combine, batch_size, learning_rate, accumulate_gradients, restore_from, run_name, checkpoint_dir, sample_every, sample_length, sample_num, multi_gpu, save_every, print_every, max_checkpoints, use_memory_saving_gradients, only_train_transformer_layers, optimizer, overwrite)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_available_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     loss = tf.reduce_mean(\n\u001b[1;32m    200\u001b[0m         input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/model.py\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(hparams, X, past, scope, gpus, reuse)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         wpe = tf.compat.v1.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n\u001b[0;32m--> 183\u001b[0;31m                              initializer=tf.compat.v1.random_normal_initializer(stddev=0.01))\n\u001b[0m\u001b[1;32m    184\u001b[0m         wte = tf.compat.v1.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n\u001b[1;32m    185\u001b[0m                              initializer=tf.compat.v1.random_normal_initializer(stddev=0.02))\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 868\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    869\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable model/wpe already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## (a) Generate Text without a prompt\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE  0m22s at 5:32 on 20210420 using 355M/fitzerald_sarahg\n",
        "\n",
        "# Try generating new text with default model parameters\n",
        "\n",
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK6djp3zpFGT"
      },
      "source": [
        "## (b) Generate Text with a Custom Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zns5HbWaGeJM"
      },
      "source": [
        "## Prompt #1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O193nNyPGiQD"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d91U0KlAGod6"
      },
      "source": [
        "## Prompt #2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDhWZLzMGod7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f8d5327-d546-4ba1-ee7e-3e0716abf689"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"Chicago Cubs win the world series in 2016\", # ENTER a custom starting prompt\n",
        "              nsamples=5,)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: 2 root error(s) found.\n  (0) Failed precondition: Attempting to use uninitialized value model/h12/attn/c_attn/w\n\t [[{{node model/h12/attn/c_attn/w/read}}]]\n\t [[strided_slice_2/_107]]\n  (1) Failed precondition: Attempting to use uninitialized value model/h12/attn/c_attn/w\n\t [[{{node model/h12/attn/c_attn/w/read}}]]\n0 successful operations.\n0 derived errors ignored.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-65d2d7bf77ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\\n\\ngpt2.generate(sess,\\n              length=1000,\\n              temperature=0.7,\\n              # Replace the \\'prefix\\' string variable with your own starting seed prompt\\n              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\\n              prefix=\"Chicago Cubs win the world series in 2016\", # ENTER a custom starting prompt\\n              nsamples=5,)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(sess, run_name, checkpoint_dir, model_name, model_dir, sample_dir, return_as_list, truncate, destination_path, sample_delim, prefix, seed, nsamples, batch_size, length, temperature, top_k, top_p, include_prefix)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             out = sess.run(output, feed_dict={\n\u001b[0;32m--> 476\u001b[0;31m                     \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcontext_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m                 })\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: 2 root error(s) found.\n  (0) Failed precondition: Attempting to use uninitialized value model/h12/attn/c_attn/w\n\t [[node model/h12/attn/c_attn/w/read (defined at /tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py:1748) ]]\n\t [[strided_slice_2/_107]]\n  (1) Failed precondition: Attempting to use uninitialized value model/h12/attn/c_attn/w\n\t [[node model/h12/attn/c_attn/w/read (defined at /tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py:1748) ]]\n0 successful operations.\n0 derived errors ignored.\n\nOriginal stack trace for 'model/h12/attn/c_attn/w/read':\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 845, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 451, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 434, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-3c6512ea0996>\", line 1, in <module>\n    get_ipython().run_cell_magic('time', '', \"\\n# NOTE: 21m46s at 4:23 on 20210414 with 124M model\\n#       11m05s at 17:30 on 20210419 with 355M model\\n#              at  5;15 on 20210420 with 355M model/fitzgerald_sarahg.txt\\n\\nsess = gpt2.start_tf_sess()\\n\\ngpt2.finetune(sess,\\n              dataset=file_name_new,\\n              model_name='355M',    # Change this if you use a different sized model\\n              steps=2000,           # Number of training epochs\\n              restore_from='fresh',\\n              run_name='run1',\\n              print_every=10,       # Print feedback of loss/avg metrics every 10 epochs\\n              sample_every=200,     # Print sample text generation every 200 epochs to see progress of model learning\\n              save_every=500        # Save learned weights/biases every 500 epochs in case of crash/interruptions\\n              )\")\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2117, in run_cell_magic\n    result = fn(magic_arg_s, cell)\n  File \"<decorator-gen-53>\", line 2, in time\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\", line 1193, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 16, in <module>\n  File \"/usr/local/lib/python3.7/dist-packages/gpt_2_simple/gpt_2.py\", line 198, in finetune\n    output = model.model(hparams=hparams, X=context, gpus=gpus)\n  File \"/usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/model.py\", line 197, in model\n    h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n  File \"/usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/model.py\", line 156, in block\n    a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n  File \"/usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/model.py\", line 132, in attn\n    c = conv1d(x, 'c_attn', n_state*3)\n  File \"/usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/model.py\", line 83, in conv1d\n    w = tf.compat.v1.get_variable('w', [1, nx, nf], initializer=tf.compat.v1.random_normal_initializer(stddev=w_init_stdev))\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\", line 1500, in get_variable\n    aggregation=aggregation)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\", line 1243, in get_variable\n    aggregation=aggregation)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\", line 567, in get_variable\n    aggregation=aggregation)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\", line 519, in _true_getter\n    aggregation=aggregation)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\", line 933, in _get_single_variable\n    aggregation=aggregation)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variables.py\", line 258, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variables.py\", line 219, in _variable_v1_call\n    shape=shape)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variables.py\", line 197, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\", line 2519, in default_variable_creator\n    shape=shape)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variables.py\", line 262, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variables.py\", line 1688, in __init__\n    shape=shape)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variables.py\", line 1872, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/array_ops.py\", line 203, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/gen_array_ops.py\", line 4239, in identity\n    \"Identity\", input=input, name=name)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpPoYtIEGiyj"
      },
      "source": [
        "## Prompt #3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o3POyYfGiyk"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"Chicago Cubs win the world series in 2016\", # ENTER a custom starting prompt\n",
        "              nsamples=5,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMNPJ4C7Gmee"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjb3P2wCGyPr"
      },
      "source": [
        "## Prompt #4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuO1Ktm3GyPs"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"Chicago Cubs win the world series in 2016\", # ENTER a custom starting prompt\n",
        "              nsamples=5,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vdpF_1xGmQI"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ISJYOaJGz_a"
      },
      "source": [
        "## Prompt #5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WglRTC_nGz_b"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"Chicago Cubs win the world series in 2016\", # ENTER a custom starting prompt\n",
        "              nsamples=5,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ5kzUgsGl6M"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8V-eL3WG1mW"
      },
      "source": [
        "## Prompt #6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2GIYdImG1mW"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"Chicago Cubs win the world series in 2016\", # ENTER a custom starting prompt\n",
        "              nsamples=5,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s5in09fGln-"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyEi922ZG25t"
      },
      "source": [
        "## Prompt #7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay-PkDD2G25u"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"Chicago Cubs win the world series in 2016\", # ENTER a custom starting prompt\n",
        "              nsamples=5,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4oDxvaAGlXK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjmZbN2wG4Pp"
      },
      "source": [
        "## Prompt #8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N7uDwHqG4Pt"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"Chicago Cubs win the world series in 2016\", # ENTER a custom starting prompt\n",
        "              nsamples=5,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JOKRmNjGlHJ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLFmDLg5G5fo"
      },
      "source": [
        "## Prompt #9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IPKt6GYG5fo"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"Chicago Cubs win the world series in 2016\", # ENTER a custom starting prompt\n",
        "              nsamples=5,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngy3xpuPHOm2"
      },
      "source": [
        "## Prompt #10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv5CbuSoHOm3"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"Chicago Cubs win the world series in 2016\", # ENTER a custom starting prompt\n",
        "              nsamples=5,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA3mQCDGIEEO"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvFWy6umIE9a"
      },
      "source": [
        "## Prompt #1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBTP9MtZIE9b"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"<Enter a custom starting prompt for GPT-2 to use as a beginning seed to generate new text>\", # ENTER a custom starting prompt\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DisBUMBEIE9c"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPc5VbyIIE9d"
      },
      "source": [
        "## Prompt #2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47BEXcKWIE9d"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"<Enter a custom starting prompt for GPT-2 to use as a beginning seed to generate new text>\", # ENTER a custom starting prompt\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONtcYye_IE9e"
      },
      "source": [
        "## Prompt #3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fjD11DOIE9e"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"<Enter a custom starting prompt for GPT-2 to use as a beginning seed to generate new text>\", # ENTER a custom starting prompt\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuAoN_sqIE9f"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bflwo2fYIE9f"
      },
      "source": [
        "## Prompt #4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgeGWue8IE9g"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"<Enter a custom starting prompt for GPT-2 to use as a beginning seed to generate new text>\", # ENTER a custom starting prompt\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOg3Ln_kIE9g"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akuo8eBnIE9h"
      },
      "source": [
        "## Prompt #5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZKzDHbMIE9h"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"<Enter a custom starting prompt for GPT-2 to use as a beginning seed to generate new text>\", # ENTER a custom starting prompt\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0SM6DzdIE9i"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIZBJdDxIE9i"
      },
      "source": [
        "## Prompt #6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T3RDCl-IE9j"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"<Enter a custom starting prompt for GPT-2 to use as a beginning seed to generate new text>\", # ENTER a custom starting prompt\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdt1ZTHdIE9j"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2F3cPJWIE9j"
      },
      "source": [
        "## Prompt #7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXg4pPx3IE9k"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"<Enter a custom starting prompt for GPT-2 to use as a beginning seed to generate new text>\", # ENTER a custom starting prompt\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8seub4VpIE9k"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am_pv_PnIE9l"
      },
      "source": [
        "## Prompt #8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcYD0AaUIE9l"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"<Enter a custom starting prompt for GPT-2 to use as a beginning seed to generate new text>\", # ENTER a custom starting prompt\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfYN_MXsIE9m"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjCUWNidIE9m"
      },
      "source": [
        "## Prompt #9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyI8WQwXIE9n"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"<Enter a custom starting prompt for GPT-2 to use as a beginning seed to generate new text>\", # ENTER a custom starting prompt\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRhdHS0YIE9n"
      },
      "source": [
        "## Prompt #10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9nuyoLNIE9o"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ENTER a custom starting prompt as a starting seed for GPT-2 to begin generating text\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=1000,\n",
        "              temperature=0.7,\n",
        "              # Replace the 'prefix' string variable with your own starting seed prompt\n",
        "              # prefix=\"It is time for world leaders to acknowledge the failure of their economic reforms. What is called for now is rather a\", # ENTER a custom starting prompt\n",
        "              prefix=\"<Enter a custom starting prompt for GPT-2 to use as a beginning seed to generate new text>\", # ENTER a custom starting prompt\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpyB-EEXozI4"
      },
      "source": [
        "## (c) Generate 10 files of Samples without a prompt (each file has 100 Samples for 1,000 Samples total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHcdHg6I2wUV"
      },
      "source": [
        "### Generate Text #1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa6p6arifSL0",
        "outputId": "bbfba010-0e4d-43cf-9d4c-614de9bb7b4a"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:  1m39s at  5:35 on 20210420 with 355M model fine-tuned on fitzgerald_sarahg.txt \n",
        "\n",
        "gen_file1 = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file1,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 5s, sys: 4.59 s, total: 1min 10s\n",
            "Wall time: 2min 44s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gul4UoJ2166"
      },
      "source": [
        "### Generate Text #2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuBP4bir2mAa",
        "outputId": "01bf28ae-91de-4d48-83d2-22d2206f02ba"
      },
      "source": [
        "%%time\n",
        "\n",
        "gen_file2 = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "# NOTE:  1m40s at  5:37 on 20210420 with 355M model fine-tuned on fitzgerald_sarahg.txt \n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file2,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 5s, sys: 4.35 s, total: 1min 10s\n",
            "Wall time: 2min 44s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x48LkFCG22wq"
      },
      "source": [
        "### Generate Text #3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In7fj6In2l4D"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:  1m40s at  5:39 on 20210420 with 355M model fine-tuned on fitzgerald_sarahg.txt \n",
        "\n",
        "gen_file3 = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file3,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_ofZE8R25I6"
      },
      "source": [
        "### Generate Text #4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCAc8Tej2lwQ"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:  1m39s at  5:42 on 20210420 with 355M model fine-tuned on fitzgerald_sarahg.txt \n",
        "\n",
        "gen_file4 = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file4,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GDKtRJc23-W"
      },
      "source": [
        "### Generate Text #5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4fqghnC2loT"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:  1m40s at  5:45 on 20210420 with 355M model fine-tuned on fitzgerald_sarahg.txt \n",
        "\n",
        "gen_file5 = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file5,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPgOXyNc27Mo"
      },
      "source": [
        "### Generate Text #6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DMzfQZ52lf3"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:  1m41s at  5:47 on 20210420 with 355M model fine-tuned on fitzgerald_sarahg.txt \n",
        "\n",
        "gen_file6 = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file6,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_K8DzjX28D8"
      },
      "source": [
        "### Generate Text #7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL4yCKta2lYj"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:  1m41s at  5:49 on 20210420 with 355M model fine-tuned on fitzgerald_sarahg.txt \n",
        "\n",
        "gen_file7 = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file7,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o51eBnJx29AV"
      },
      "source": [
        "### Generate Text #8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpVvykOh2lQg"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:  1m41s at  5:51 on 20210420 with 355M model fine-tuned on fitzgerald_sarahg.txt \n",
        "\n",
        "gen_file8 = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file8,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x2A0HmL294d"
      },
      "source": [
        "### Generate Text #9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc6z_hfS2lHo"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:  1m39s at  5:53 on 20210420 with 355M model fine-tuned on fitzgerald_sarahg.txt \n",
        "\n",
        "gen_file9 = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file9,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyei1XNT2-wk"
      },
      "source": [
        "### Generate Text #10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hyRKYac2k_E"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:  1m39s at  5:42 on 20210420 with 355M model fine-tuned on fitzgerald_sarahg.txt \n",
        "\n",
        "gen_file10 = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file10,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Oykk0Prmevx"
      },
      "source": [
        "### Zip all 10 files of generated samples (100 each) and Download zip fille"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpIaOHT_tlfe"
      },
      "source": [
        "!ls gpt2_*.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZfHoIi8keVe"
      },
      "source": [
        "zipfile_name = 'gpt2_gentext_' + file_name.split('.')[0].strip() + '.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO9H3JCFmUbL"
      },
      "source": [
        "!zip -r $zipfile_name gpt2_gentext_* # gen_file1 gen_file2 gen_file3 gen_file4 gen_file5 gen_file6 gen_file7 gen_file8 gen_file9 gen_file10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWHCyAyZmjTU"
      },
      "source": [
        "# Download the zip archive with all 1000 Samples (100 per file)\n",
        "\n",
        "# NOTE: YOU MUST CLICK [OK] IN THE POP-UP DIALOG BOX that appears to permanently save your work\n",
        "#       if you don't do this you will lose all your work!!!\n",
        "\n",
        "files.download(zipfile_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HTNzL_Uugoh"
      },
      "source": [
        "# **END OF NOTEBOOK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX"
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}